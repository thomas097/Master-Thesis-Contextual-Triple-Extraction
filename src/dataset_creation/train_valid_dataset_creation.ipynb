{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "257a889f",
   "metadata": {},
   "source": [
    "## Dataset Creation with Circa, PersonaChat and DailyDialogs\n",
    "\n",
    "In this notebook we will be creating a stratified training and test set. For this we will sample data from three datasets: the PersonaChat dataset, DailyDialogs and Google's Circa dataset (for binary yes/no questions and implicit answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187408c0",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5786ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "PART = 'valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1aed2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_personachat(path, part='train', k=1000, max_chars=200):\n",
    "    triplets = []\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)[part]\n",
    "        \n",
    "        # Loop through utterances in dialogs\n",
    "        for i, item in enumerate(data):\n",
    "            history = item['utterances'][-1]['history']\n",
    "            personachat_id = 'personachat-{}-{}'.format(part, str(i).zfill(6))\n",
    "            \n",
    "            # Extract triplets\n",
    "            for j in range(len(history) - 2):\n",
    "                triplet = history[j:j + 3]\n",
    "                \n",
    "                # Verify that it is a question and a response\n",
    "                if not triplet[1].endswith('?') or '?' in triplet[2]:\n",
    "                    continue\n",
    "                \n",
    "                # Strip __SILENCE__ off\n",
    "                if '__' not in ' '.join(triplet) and '' not in triplet:\n",
    "                    triplets.append((personachat_id, triplet))\n",
    "             \n",
    "    # Limit length of triplets\n",
    "    triplets = [(id_, tr, None) for id_, tr in triplets if len(' '.join(tr)) < max_chars]\n",
    "                \n",
    "    return random.sample(triplets, k)\n",
    "\n",
    "\n",
    "def load_dailydialogs(path, part='train', k=1000, max_chars=200):\n",
    "    triplets = []\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            history = [turn.strip() for turn in line.split('__eou__')]\n",
    "            daily_id = 'daily_dialogs-{}-{}'.format(part, str(i).zfill(6))\n",
    "                        \n",
    "            # Split punct off from tokens\n",
    "            history = [re.sub(\"â€™\", \"'\", h) for h in history] # weird apos\n",
    "            history = [' '.join(re.findall(r\"[\\w']+|[.,!?;]+\", h)) for h in history]\n",
    "            \n",
    "            for j in range(len(history) - 2):\n",
    "                triplet = history[j:j + 3]\n",
    "                \n",
    "                # Verify that it is a question and a response\n",
    "                if not triplet[1].endswith('?') or '?' in triplet[2]:\n",
    "                    continue\n",
    "                    \n",
    "                if '' not in triplet:\n",
    "                    triplets.append((daily_id, triplet))\n",
    "                \n",
    "    # Limit length of triplets\n",
    "    triplets = [(id_, tr, None) for id_, tr in triplets if len(' '.join(tr)) < max_chars]\n",
    "                \n",
    "    # Shuffle deterministcally and create 50/50 train-test sets\n",
    "    random.Random(1).shuffle(triplets) # shuffle deterministically\n",
    "    n = len(triplets) // 2\n",
    "    dataset = triplets[:n] if part == 'train' else triplets[n:]\n",
    "    return random.sample(dataset, k)\n",
    "\n",
    "\n",
    "def load_circa(path, part='train', k=1000, max_chars=200):\n",
    "    triplets = []\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    for _, row in df.iterrows():\n",
    "        # Assign ID to question and answer\n",
    "        circa_id = 'circa-{}-{}'.format(part, str(row['id']).zfill(6))\n",
    "        question = row['question-X'].lower()\n",
    "        answer = row['answer-Y'].lower()\n",
    "        label = str(row['goldstandard1']).lower()\n",
    "        \n",
    "        # Split punct off from tokens\n",
    "        question = ' '.join(re.findall(r\"[\\w']+|[.,!?;]+\", question))\n",
    "        answer = ' '.join(re.findall(r\"[\\w']+|[.,!?;]+\", answer))\n",
    "        \n",
    "        # Extract alternative answers if there are any\n",
    "        answers = []\n",
    "        if label in ['yes', 'no']:           \n",
    "            triplets.append((circa_id, [question, answer], label))\n",
    "            triplets.append((circa_id, [question, label], label))\n",
    "            triplets.append((circa_id, [question, '{} , {}'.format(label, answer)], label))\n",
    "            \n",
    "    # Limit length of triplets\n",
    "    triplets = [(id_, tr, label) for id_, tr, label in triplets if len(' '.join(tr)) < max_chars]\n",
    "           \n",
    "    # Shuffle deterministcally and create 50/50 train-test sets\n",
    "    random.Random(1).shuffle(triplets) # shuffle deterministically\n",
    "    n = len(triplets) // 2\n",
    "    dataset = triplets[:n] if part == 'train' else triplets[n:]\n",
    "    return random.sample(dataset, k)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6eda8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "personachat = load_personachat('originals/personachat_self_original.json', part=PART)\n",
    "circa = load_circa('originals/circa-data.tsv', part=PART)\n",
    "dailydialogs = load_dailydialogs('originals/dialogues_text.txt', part=PART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45984b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "pprint(len(personachat))\n",
    "pprint(len(circa))\n",
    "pprint(len(dailydialogs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aedc517b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('personachat-valid-000446',\n",
      " ['oh i remember horse back riding when i was a kid . now my kids love it .',\n",
      "  'do you have any horses ? are they in high school ?',\n",
      "  \"i've one we keep at a stable in kentucky 10 , 9 and 7\"],\n",
      " None)\n",
      "\n",
      "('circa-valid-007564',\n",
      " ['did you read any of the best sellers ?', \"i don't have time to read .\"],\n",
      " 'no')\n",
      "\n",
      "('daily_dialogs-valid-012426',\n",
      " ['I stabbed his belly three times .',\n",
      "  'Did you know hat your actions might cause serous injuries or death ?',\n",
      "  \"I knew , but I couldn't myself .\"],\n",
      " None)\n"
     ]
    }
   ],
   "source": [
    "pprint(random.choice(personachat))\n",
    "print()\n",
    "pprint(random.choice(circa))\n",
    "print()\n",
    "pprint(random.choice(dailydialogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb2367",
   "metadata": {},
   "source": [
    "### Augmenting Circa with additional context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8811324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa5998c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_personachat_as_context(path, part='train'):\n",
    "    utterances = list()\n",
    "    inv_index = defaultdict(set)\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)[part]\n",
    "        \n",
    "        # Loop through utterances in dialogs\n",
    "        j = 0\n",
    "        for item in data:\n",
    "            history = item['utterances'][-1]['history']\n",
    "            \n",
    "            for utterance in history:\n",
    "                # Record utterance\n",
    "                if '?' not in utterance:\n",
    "                    utterances.append(utterance)\n",
    "\n",
    "                    # Index tokens in utterance\n",
    "                    tokens = re.findall(r\"[\\w']+|[.,!?;]\", utterance.lower())\n",
    "                    for token in tokens:\n",
    "                        inv_index[token].add(j)\n",
    "                    j += 1\n",
    "                \n",
    "    return utterances, inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20896064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turn with \"whittling\": i just got done watching a horror movie\n",
      "turns with \"whittling\": set()\n"
     ]
    }
   ],
   "source": [
    "personachat_contexts, personachat_index = load_personachat_as_context('originals/personachat_self_original.json', part=PART)\n",
    "print('turn with \"whittling\":', personachat_contexts[1])\n",
    "print('turns with \"whittling\":', personachat_index['whittling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9883ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible contexts: 10127\n"
     ]
    }
   ],
   "source": [
    "print('Possible contexts:', len(personachat_contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a93ba975",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:06<00:00, 154.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Defines dependency relations for topic words\n",
    "TOPIC_DEPS = ['nsubj', 'nsubjpass', 'dobj', 'pobj']\n",
    "TOPIC_POS = ['NOUN', 'PROPN']\n",
    "\n",
    "circa_augmented = []\n",
    "for id_, (question, answer), label in tqdm(circa):\n",
    "    # Identify spans of tokens in the question that determine its topic\n",
    "    doc = nlp(question)\n",
    "    topics = [t for t in doc if t.dep_ in TOPIC_DEPS and t.pos_ in TOPIC_POS]\n",
    "    topic_tokens = [' '.join([t.lower_ for t in span.subtree]) for span in topics]\n",
    "    topic_tokens += [t.lower_ for t in doc if t.pos_ in TOPIC_POS]\n",
    "    \n",
    "    # Find utterances that match\n",
    "    matches = set()\n",
    "    for topic in topic_tokens:\n",
    "        if topic in personachat_index:\n",
    "            for i in personachat_index[topic]:\n",
    "                matches.add(personachat_contexts[i])\n",
    "                \n",
    "    # Score matches\n",
    "    if len(matches) > 2:\n",
    "        best_match = max(matches, key=lambda m: sum([1 if t in m else 0 for t in topic_tokens]))\n",
    "        circa_augmented.append((id_, (best_match, question, answer), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73077fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('circa-valid-019475',\n",
      " ('cool . . . when i have a break from my horse farm i like to watch movies',\n",
      "  'do you like to watch movies ?',\n",
      "  'yes'),\n",
      " 'yes')\n"
     ]
    }
   ],
   "source": [
    "pprint(random.choice(circa_augmented))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97be3ca",
   "metadata": {},
   "source": [
    "# Creating a stratified dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba4962",
   "metadata": {},
   "source": [
    "## Ellipsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58205aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ellipsis_subject(rdoc):\n",
    "    subjects = [token for token in rdoc if 'subj' in token.dep_]  # nsubjpass, nsubj, etc.\n",
    "    predicate = [token for token in rdoc if token.dep_ == 'ROOT' and token.pos_ in ['VERB', 'AUX']]\n",
    "    return not subjects and predicate\n",
    "\n",
    "def ellipsis_predicate(rdoc):\n",
    "    predicate = [token for token in rdoc if token.dep_ == 'ROOT' and token.pos_ in ['VERB', 'AUX']]\n",
    "    return not predicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1018e",
   "metadata": {},
   "source": [
    "## Response types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57565ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIRM = ['yes', 'sure', 'maybe', 'yeah', 'yea', 'yup']\n",
    "DENY = ['no', 'maybe', 'nah', 'nope', 'neh']\n",
    "\n",
    "\n",
    "def is_confirm(rdoc):\n",
    "    if len(rdoc) == 1:\n",
    "        return rdoc[0].lower_ in CONFIRM \n",
    "    elif len(rdoc) == 2:\n",
    "        return rdoc[0].lower_ in CONFIRM and rdoc[1].dep_ == 'PUNCT'\n",
    "    return False\n",
    "\n",
    "def is_deny(rdoc):\n",
    "    if len(rdoc) == 1:\n",
    "        return rdoc[0].lower_ in DENY \n",
    "    elif len(rdoc) == 2:\n",
    "        return rdoc[0].lower_ in DENY and rdoc[1].dep_ == 'PUNCT'\n",
    "    return False\n",
    "    \n",
    "def is_confirm_with_elaboration(rdoc):\n",
    "    return not is_confirm(rdoc) and rdoc[0].lower_ in CONFIRM\n",
    "\n",
    "def is_deny_elaboration(rdoc):\n",
    "    return not is_confirm(rdoc) and rdoc[0].lower_ in DENY \n",
    "\n",
    "def is_implicit_confirm(label, rdoc):\n",
    "    return label == 'yes' and not is_confirm(rdoc) and not is_confirm_with_elaboration(rdoc)\n",
    "\n",
    "def is_implicit_deny(label, rdoc):\n",
    "    return label == 'no' and not is_deny(rdoc) and not is_deny_elaboration(rdoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377e6ed",
   "metadata": {},
   "source": [
    "## Question type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e457d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased found in cache. Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at C:\\Users\\Uw naam/.dialog-tag/models\\distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['dropout_59']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\Uw naam/.dialog-tag/models\\distilbert-base-uncased and are newly initialized: ['dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from dialog_tag import DialogTag\n",
    "\n",
    "model = DialogTag('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad66bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_yesno_question(tag):\n",
    "    return tag == 'Yes-No-Question'\n",
    "\n",
    "def is_wh_question(tag):\n",
    "    return tag == 'Wh-Question'\n",
    "\n",
    "def is_declarative_yesno_question(tag):\n",
    "    return tag == 'Declarative Yes-No-Question'\n",
    "\n",
    "def is_open_question(tag):\n",
    "    return tag == 'Open-Question'\n",
    "\n",
    "def is_rhetorical_question(tag):\n",
    "    return tag == 'Rhetorical-Questions'\n",
    "\n",
    "def is_declarative_wh_question(tag):\n",
    "    return tag == 'Declarative Wh-Question'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeaceb2",
   "metadata": {},
   "source": [
    "## Coreferring expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcc53535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_reflexive_pronoun(rdoc):\n",
    "    return [t for t in rdoc if t.lower_ in ['myself', 'yourself', 'ourselves', 'himself', 'herself', 'themselves']]\n",
    "\n",
    "def has_possessive_pronoun(rdoc):\n",
    "    return [t for t in rdoc if t.tag_ == 'PRP$' and t.lower_ in ['my', 'mine', 'your', 'our', 'his', 'her', 'its']]\n",
    "\n",
    "def has_demonstrative_dets(rdoc):\n",
    "    return [t for i, t in enumerate(rdoc[:-1]) if t.lower_ in ['this', 'these', 'that'] and rdoc[i + 1].pos_ != 'NOUN']\n",
    "\n",
    "def has_singular_personal_pronoun(rdoc):\n",
    "    return [t for t in rdoc if t.tag_ == 'PRP' and t.lower_ in ['i', 'me', 'you', 'it', 'he', 'she']]\n",
    "\n",
    "def has_plural_personal_pronoun(rdoc):\n",
    "    return [t for t in rdoc if t.tag_ == 'PRP' and t.lower_ in ['we', 'they', 'them']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada9b1f5",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a18b626f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/2671 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x207d8aa6d60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x207d8aa6d60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                               | 1/2671 [00:05<4:13:18,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                               | 2/2671 [00:07<2:28:25,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2671/2671 [04:06<00:00, 10.86it/s]\n"
     ]
    }
   ],
   "source": [
    "triplets = []\n",
    "\n",
    "used_data = personachat + dailydialogs + circa_augmented\n",
    "\n",
    "for id_, triplet, label in tqdm(used_data):\n",
    "    triplet_info = {'id': id_, 'triplet': '<eos>'.join(triplet), 'categories': []}\n",
    "    \n",
    "    qdoc, rdoc = nlp(triplet[1]), nlp(triplet[2])\n",
    "    qtag = model.predict_tag(triplet[1])\n",
    "    \n",
    "    # Ellipsis\n",
    "    if ellipsis_subject(rdoc):\n",
    "        triplet_info['categories'].append('ellipsis_subj')\n",
    "        \n",
    "    if ellipsis_predicate(rdoc):\n",
    "        triplet_info['categories'].append('ellipsis_pred')\n",
    "        \n",
    "    # Implicit response\n",
    "    if is_implicit_confirm(label, rdoc):\n",
    "        triplet_info['categories'].append('act_impl_confirm')\n",
    "        \n",
    "    if is_implicit_deny(label, rdoc):\n",
    "        triplet_info['categories'].append('act_impl_deny')\n",
    "        \n",
    "    # Coreferring expressions\n",
    "    if has_reflexive_pronoun(rdoc):\n",
    "        triplet_info['categories'].append('reflexive_prons')\n",
    "\n",
    "    if has_possessive_pronoun(rdoc):\n",
    "        triplet_info['categories'].append('possessive_prons')\n",
    "\n",
    "    if has_demonstrative_dets(rdoc):\n",
    "        triplet_info['categories'].append('demonstrative_dets')\n",
    "\n",
    "    if has_singular_personal_pronoun(rdoc):\n",
    "        triplet_info['categories'].append('singular_personal_prons')\n",
    "\n",
    "    if has_plural_personal_pronoun(rdoc):\n",
    "        triplet_info['categories'].append('plural_personal_prons')\n",
    "        \n",
    "    # (Response) Dialog acts\n",
    "    if is_confirm(rdoc):\n",
    "        triplet_info['categories'].append('act_confirm')\n",
    "        \n",
    "    if is_deny(rdoc):\n",
    "        triplet_info['categories'].append('act_deny')\n",
    "        \n",
    "    if is_confirm_with_elaboration(rdoc):\n",
    "        triplet_info['categories'].append('act_confirm_elaborate')\n",
    "        \n",
    "    if is_deny_elaboration(rdoc):\n",
    "        triplet_info['categories'].append('act_deny_elaborate')\n",
    "        \n",
    "    # (Question) Dialog acts\n",
    "    if is_yesno_question(qtag):\n",
    "        triplet_info['categories'].append('yes_no_question')\n",
    "        \n",
    "    if is_wh_question(qtag):\n",
    "        triplet_info['categories'].append('wh_question')\n",
    "        \n",
    "    if is_declarative_yesno_question(qtag):\n",
    "        triplet_info['categories'].append('declarative_yes_no_question')\n",
    "        \n",
    "    if is_open_question(qtag):\n",
    "        triplet_info['categories'].append('open_question')\n",
    "        \n",
    "    if is_rhetorical_question(qtag):\n",
    "        triplet_info['categories'].append('rhetorical_question')\n",
    "        \n",
    "    if is_declarative_wh_question(qtag):\n",
    "        triplet_info['categories'].append('declarative_wh_question')\n",
    "    \n",
    "    # Sentence length\n",
    "    triplet_info['categories'].append('length_{}'.format(len(rdoc)))\n",
    "        \n",
    "    triplets.append(triplet_info)\n",
    "        \n",
    "        \n",
    "with open('{}.json'.format(PART), 'w', encoding='utf-8') as file:\n",
    "    json.dump(triplets, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944152d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40502886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
